{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, CenterCrop, Normalize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import DataLoader\n",
    "from barbar import Bar\n",
    "from parse_image import *\n",
    "import pandas as pd\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import open_clip\n",
    "GPU = 4\n",
    "data_path = \"/path/to/csv.csv\" #ComVG or SVO_Probes csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace with other model version\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-L-14')\n",
    "model.cuda(GPU).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ComOpenCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subimage_score_embedding(image, text):\n",
    "    if text:\n",
    "        image = preprocess(image)\n",
    "        text_input = tokenizer([text]).cuda(GPU)\n",
    "        image_input = torch.tensor(np.stack([image])).cuda(GPU)\n",
    "        with torch.no_grad():\n",
    "            image_embed = model.encode_image(image_input).float()\n",
    "            text_embed = model.encode_text(text_input).float()\n",
    "        score = text_embed @ image_embed.T\n",
    "        return image_embed, score\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comclip_one_pair(row, caption, image_id):\n",
    "    image = preprocess(read_image(image_id))\n",
    "    text_input = tokenizer(row.sentence).cuda(GPU)\n",
    "    image_input = torch.tensor(np.stack([image])).cuda(GPU)\n",
    "    with torch.no_grad():\n",
    "        original_image_embed = model.encode_image(image_input).float()\n",
    "        original_text_embed = model.encode_text(text_input).float()\n",
    "\n",
    "    svo = row.pos_triplet.split(\",\")\n",
    "    subj, verb, obj = svo[0], svo[1], svo[-1]\n",
    "    object_images, matched_json = create_sub_image_obj(row.sentence_id, image_id)\n",
    "    relation_images, relation_words = create_relation_object(object_images, subj, verb, obj, image_id, matched_json)\n",
    "    if relation_images and relation_words:\n",
    "        for relation_image, word in zip(relation_images, relation_words):\n",
    "            object_images[word] = relation_image\n",
    "\n",
    "    image_embeds = []\n",
    "    image_scores = []\n",
    "    for key, sub_image in object_images.items():\n",
    "        image_embed, image_score = subimage_score_embedding(sub_image, key)\n",
    "        if image_embed is not None and image_score is not None:\n",
    "            image_embeds.append(image_embed)\n",
    "            image_scores.append(image_score)\n",
    "    #regularize the scores\n",
    "    similarity = normalize_tensor_list(image_scores)\n",
    "    for score, image in zip(similarity, image_embeds):\n",
    "        original_image_embed += score * image\n",
    "    image_features = original_image_embed / original_image_embed.norm(dim=-1, keepdim=True).float()\n",
    "    text_features = original_text_embed /original_text_embed.norm(dim=-1, keepdim=True).float()\n",
    "    similarity = text_features.detach().cpu().numpy() @ image_features.detach().cpu().numpy().T\n",
    "    return similarity, object_images, relation_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_one_row(idx, row):\n",
    "    result_pos = comclip_one_pair(row, row.sentence, row.pos_image_id)[0].item()\n",
    "    result_neg = comclip_one_pair(row, row.sentence, row.neg_image_id)[0].item()\n",
    "    result = {\"id\" : idx, \"pos_score\": result_pos, \"neg_score\": result_neg}\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comclip_score = []\n",
    "for idx, row in data.iterrows():\n",
    "    score = compute_one_row(idx, row)\n",
    "    comclip_score.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCLIP baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_id = [] ## if cosine similarity to positive image larger: 0, if not: 1\n",
    "for idx, row in data.iterrows():\n",
    "    try:\n",
    "        text = row.sentence\n",
    "        image_pos = preprocess(Image.open(os.path.join(skimage.data_dir, image_path+str(row.pos_image_id) + \".jpg\")).convert(\"RGB\"))\n",
    "        image_neg = preprocess(Image.open(os.path.join(skimage.data_dir, image_path+str(row.neg_image_id) + \".jpg\")).convert(\"RGB\"))\n",
    "        images = [image_pos, image_neg]\n",
    "        image_input = torch.tensor(np.stack(images)).cuda(5)\n",
    "        text_tokens = tokenizer([text]).cuda(5)\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input).float()\n",
    "            text_features = model.encode_text(text_tokens).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity =text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "        matched_id.append(np.argmax(similarity))\n",
    "    except UnidentifiedImageError:\n",
    "        matched_id.append(\"image_failed\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('clip-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88beffe3055ef9797590ee073211e9aee9c8621e04d2e6cfacd27b0cbc9abf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
